{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CNN great for image modeling, does not have to flatten layers, 2d and 3d,convolutions goal to locate features, ex 3x3 conv kernal, edges, curves, corners into more complex, circles squares, slide window over image(condensing), pooling (max)(Value),Drastically simp. image, multiple layers of convolutions, combo of features is first layer, then combo of circles and squares and so on and so forth, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np \n",
    "from tqdm import tqdm #low overhead, tqdm uses smart algorithms to predict the remaining time and to skip unnecessary iteration displays, which allows for a negligible overhead in most cases.\n",
    "\n",
    "REBUILD_DATA = True #time saver\n",
    "\n",
    "\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50 #shift, pad, rotate, flip \n",
    "    CATS = \"Cat\"\n",
    "    DOGS = \"Dog\"\n",
    "    TESTING = \"Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) #color more data/relevant feature?\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(training_data[3][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50) #ith any element\n",
    "        self._to_linear = None\n",
    "        self.convs(x) #run forward pass to deter stl #\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2, \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x #pytorch no abstract flattening\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1) #distrbuted for cat and dog \n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimization \n",
    "- Adam \n",
    "- Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9bddd554b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mVAL_PCT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m  \u001b[0;31m# lets reserve 10% of our data for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mVAL_PCT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#valid index used for slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT) #valid index used for slicing\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[:-val_size] #\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1 #modify this for storage issues, then tweak layers and model\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        net.zero_grad() #model.zero_grad-used when multiple NN and Optimizers/opimizer.zero_grad\n",
    "\n",
    "        outputs = net(batch_X) \n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update, have everything activated when evaluating model .eval .train same in tensorflow\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "renting gpu in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read statistics of model to determine further testing/visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kuda toolkit, linode, gpu cloud services. ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_X), len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def__init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn=Conv2d(1, 32, 5) #2d scans and models \n",
    "        self.conv2 = nn=Conv2d(32, 64, 5)    #1d temportal and spatial \n",
    "        self.conv3 = nn=Conv2d(64, 128, 5)  #keras has flatten function or method, simplififed. (pass date thru to find out shape.) linear/dense layers,\n",
    "        \n",
    "        x = torch.randn(50,50).view(-1,1,50,50) #data thru three layers, output = check shape. mulp. shape\n",
    "        self._to_linear = none\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512,2 )\n",
    "        def convs(self, x):\n",
    "            x = F.max_pool2d(F.relu(self.con1(x)), (2,2))\n",
    "            x = F.max_pool2d(F.relu(self.con1(x)), (2,2))\n",
    "            x = F.max_pool2d(F.relu(self.con1(x)), (2,2)) #finding value \n",
    "            \n",
    "            print(x[0].shape)\n",
    "            if self._to_linear is none:\n",
    "                self._to_linear = x[0].shape[0].shape[1]*x[0].shape[2]\n",
    "            return x\n",
    "            \n",
    "            def forward(self,x):\n",
    "                x = self.convs(x)\n",
    "                x = x.view(-1, self._to_linear) #shape to flatten after comms in intialization\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = self.fc2(x)\n",
    "                return x #analyze and print whats needed\n",
    "net = Net()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "def train(net):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    BATCH_SIZE = 5\n",
    "    EPOCHS = 15\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            net.zero_grad()\n",
    "\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "        \n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 values in sample,outof sample loss and accuracy, vital for model analysis\n",
    "#granular testing\n",
    "def fwd_pass(X,y, train=False): #wont learn if set to true\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "    outputs = net(X)\n",
    "    matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs,y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "    \n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(size=32):\n",
    "    random_start = np.random.randint(len(test_X)-size)\n",
    "    X, y = test_X[random_start:random_start+size], test_y[random_start:random_start+size] #cant shuffle a zip\n",
    "    val_acc, val_loss = fwd_pass(X.view(-1,1,50,50).to(device),y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc,val_loss = test(size=100)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop, while tracking in sample accurancy and loss to out of sample\n",
    "#tensorboard(x) in pytorch (graph scalars)]\n",
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                f.write(f\"{MODEL_NAME},{round(time.time(),3)},in_sample,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i == 5:\n",
    "                    break\n",
    "                break\n",
    "                \n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = \"model-1570490221\" # grab whichever model name you want here. We could also just reference the MODEL_NAME if you're in a notebook still.\n",
    "\n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, sample_type, acc, loss = c.split(\",\")\n",
    "\n",
    "            times.append(timestamp)\n",
    "            accuracies.append(acc)\n",
    "            losses.append(loss)\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"in_samp_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "    ax2.plot(times,losses, label=\"in_samp_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "create_acc_loss_graph(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(size=32):\n",
    "    X, y = test_X[:size], test_y[:size]\n",
    "    val_acc, val_loss = fwd_pass(X.view(-1, 1, 50, 50).to(device), y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc, val_loss = test(size=100)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                #f.write(f\"{MODEL_NAME},{round(time.time(),3)},train,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i % 10 == 0:\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = MODEL_NAME #\"model-1570499409\" # grab whichever model name you want here. We could also just reference the MODEL_NAME if you're in a notebook still.\n",
    "\n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, acc, loss, val_acc, val_loss = c.split(\",\")\n",
    "\n",
    "            times.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"acc\")\n",
    "    ax1.plot(times, val_accs, label=\"val_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "    ax2.plot(times,losses, label=\"loss\")\n",
    "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "create_acc_loss_graph(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "print(MODEL_NAME)\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 30\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                #f.write(f\"{MODEL_NAME},{round(time.time(),3)},train,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i % 50 == 0:\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)},{epoch}\\n\")\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = \"model-1570499915\" # grab whichever model name you want here. We could also just reference the MODEL_NAME if you're in a notebook still.\n",
    "\n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, acc, loss, val_acc, val_loss, epoch = c.split(\",\")\n",
    "\n",
    "            times.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"acc\")\n",
    "    ax1.plot(times, val_accs, label=\"val_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "    ax2.plot(times,losses, label=\"loss\")\n",
    "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "create_acc_loss_graph(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
